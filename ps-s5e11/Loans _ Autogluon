{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"L4","authorship_tag":"ABX9TyOY8trcU1YDyKKFmzV1P6Cf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# AutoGluon Tabular v1.4 Extreme Preset and Ensemble Models\n","\n","This notebook uses AutoGluon Tabular v1.4 with the Extreme preset to tackle the loan default prediction task in Kaggle Playground Series S5E11.\n","\n","- Competition overview:  \n","  https://www.kaggle.com/competitions/playground-series-s5e11/overview\n","\n","- Related discussion threads for this approach:  \n","  - Discussion 617692 (AutoGluon / ensemble strategy, results, and tips):  \n","    https://www.kaggle.com/competitions/playground-series-s5e11/discussion/617692  \n","  - Discussion 614986 (additional approaches and ideas):  \n","    https://www.kaggle.com/competitions/playground-series-s5e11/discussion/614986  \n","\n","---\n","\n","## What AutoGluon Tabular Does\n","\n","AutoGluon Tabular is an AutoML framework for structured tabular data. It:\n","\n","- Automatically handles preprocessing  \n","  - Infers feature types (numeric, categorical, text, datetime)  \n","  - Deals with missing values and categorical encoding  \n","\n","- Trains a portfolio of models  \n","  - Gradient boosted trees (LightGBM, XGBoost, CatBoost)  \n","  - Linear models, k-nearest neighbors, random forests  \n","  - Neural networks and tabular foundation models  \n","\n","- Uses bagging and stacking  \n","  - Bagging: multiple folds or resamples per model to get robust out-of-fold predictions  \n","  - Stacking: higher level models learn how to combine predictions from lower level models  \n","\n","- Builds a final weighted ensemble that often outperforms any single model\n","\n","The goal is to replace manual model selection and blending with a strong automatically tuned ensemble.\n","\n","---\n","\n","## The Extreme Preset (v1.4)\n","\n","The Extreme preset is the highest accuracy mode of AutoGluon Tabular v1.4 for small and medium sized tabular datasets. It:\n","\n","- Uses meta-learned hyperparameters from large meta-benchmarks  \n","- Trains more models and uses deeper bagging and stacking than the best_quality preset  \n","- Adds several state of the art tabular models to the ensemble:\n","  - Mitra\n","  - TabPFNv2\n","  - TabICL\n","  - RealMLP\n","  - TabM\n","\n","For Kaggle style problems, Extreme tries to squeeze out the best ROC AUC by combining many diverse, strong base learners.\n","\n","---\n","\n","## Key New Models in the Ensemble (v1.4)\n","\n","These models are important additions in AutoGluon Tabular v1.4 and are used inside the Extreme preset.\n","\n","### Mitra\n","\n","- Tabular foundation model with a transformer style architecture  \n","- Pretrained on large amounts of synthetic tabular data  \n","- Designed to generalize well after fine tuning and capture rich feature interactions  \n","\n","### TabPFNv2\n","\n","- Based on prior data fitted networks (TabPFN)  \n","- Acts like an in context learning model for tabular data  \n","- Very strong on small datasets, where traditional deep models can overfit  \n","\n","### TabICL\n","\n","- Tabular foundation model specialized for in context learning on larger datasets  \n","- Treats tabular prediction as \"examples plus query row produce prediction\"  \n","- Focused on classification tasks and can scale better than TabPFNv2 in row count  \n","\n","### RealMLP\n","\n","- High performance MLP based tabular model  \n","- Uses architectural and training tricks tuned on many datasets  \n","- Often competitive with gradient boosted trees, especially when the signal is friendly to neural networks  \n","\n","### TabM\n","\n","- Parameter efficient ensemble of MLPs inside a single network  \n","- Produces multiple predictions per sample internally and combines them  \n","- Captures ensemble diversity without training many separate models  \n","\n","---\n","\n","## Why Use This Stack For This Competition?\n","\n","For this loan default prediction task:\n","\n","- Classic models in AutoGluon (gradient boosted trees, linear models, random forests) provide solid baselines  \n","- The foundation models and advanced MLPs (Mitra, TabPFNv2, TabICL, RealMLP, TabM) capture more complex non linear structure  \n","- The Extreme preset bags and stacks all of these, then learns the best combination based on validation ROC AUC\n","\n","This gives a strong meta learned ensemble that can outperform a manually tuned blend of logistic regression, LightGBM, and XGBoost, while keeping the training code relatively simple.\n","\n","\n","## What AutoGluon Tabular Does\n","\n","AutoGluon Tabular is an AutoML framework for structured tabular data. It:\n","\n","- Automatically handles preprocessing  \n","  - Infers feature types (numeric, categorical, text, datetime)  \n","  - Deals with missing values and categorical encoding  \n","\n","- Trains a portfolio of models  \n","  - Gradient boosted trees (LightGBM, XGBoost, CatBoost)  \n","  - Linear models, k-nearest neighbors, random forests  \n","  - Neural networks and tabular foundation models  \n","\n","- Uses bagging and stacking  \n","  - Bagging: multiple folds or resamples per model to get robust out-of-fold predictions  \n","  - Stacking: higher level models learn how to combine predictions from lower level models  \n","\n","- Builds a final weighted ensemble that often outperforms any single model\n","\n","The goal is to replace manual model selection and blending with a strong automatically tuned ensemble.\n","\n","---\n","\n","## The Extreme Preset (version 1.4)\n","\n","The Extreme preset is the highest accuracy mode of AutoGluon Tabular version 1.4 for small and medium sized tabular datasets. It:\n","\n","- Uses meta-learned hyperparameters from large meta-benchmarks  \n","- Trains more models and uses deeper bagging and stacking than the best_quality preset  \n","- Adds several state of the art tabular models to the ensemble:\n","  - Mitra\n","  - TabPFNv2\n","  - TabICL\n","  - RealMLP\n","  - TabM\n","\n","For Kaggle style problems, Extreme tries to squeeze out the best ROC AUC by combining many diverse, strong base learners.\n","\n","---\n","\n","## Key New Models in the Ensemble (version 1.4)\n","\n","These models are important additions in AutoGluon Tabular version 1.4 and are used inside the Extreme preset.\n","\n","### Mitra\n","\n","- Tabular foundation model with a transformer style architecture  \n","- Pretrained on large amounts of synthetic tabular data  \n","- Designed to generalize well after fine tuning and capture rich feature interactions  \n","\n","### TabPFNv2\n","\n","- Based on prior data fitted networks (TabPFN)  \n","- Acts like an in context learning model for tabular data  \n","- Very strong on small datasets, where traditional deep models can overfit  \n","\n","### TabICL\n","\n","- Tabular foundation model specialized for in context learning on larger datasets  \n","- Treats tabular prediction as \"examples plus query row produce prediction\"  \n","- Focused on classification tasks and can scale better than TabPFNv2 in row count  \n","\n","### RealMLP\n","\n","- High performance MLP based tabular model  \n","- Uses architectural and training tricks tuned on many datasets  \n","- Often competitive with gradient boosted trees, especially when the signal is friendly to neural networks  \n","\n","### TabM\n","\n","- Parameter efficient ensemble of MLPs inside a single network  \n","- Produces multiple predictions per sample internally and combines them  \n","- Captures ensemble diversity without training many separate models  \n","\n","---\n","\n","## Why Use This Stack For This Competition?\n","\n","For this loan default prediction task:\n","\n","- Classic models in AutoGluon (gradient boosted trees, linear models, random forests) provide solid baselines  \n","- The foundation models and advanced MLPs (Mitra, TabPFNv2, TabICL, RealMLP, TabM) capture more complex non linear structure  \n","- The Extreme preset bags and stacks all of these, then learns the best combination based on validation ROC AUC\n","\n","This gives a strong meta learned ensemble that can outperform a manually tuned blend of logistic regression, LightGBM, and XGBoost, while keeping the training code relatively simple.\n"],"metadata":{"id":"BjCNmeUKq6d9"}},{"cell_type":"code","source":["from google.colab import files\n","\n","# Upload kaggle.json from your local machine\n","files.upload()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":90},"id":"oTkvinegrggN","executionInfo":{"status":"ok","timestamp":1763396982213,"user_tz":480,"elapsed":220085,"user":{"displayName":"Jordan Limperis","userId":"13371126362984529793"}},"outputId":"c53dcd1c-4598-4884-e45c-c040694b1f33"},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-908d8c36-cba2-4264-a30a-244184066401\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-908d8c36-cba2-4264-a30a-244184066401\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving kaggle.json to kaggle (1).json\n"]},{"output_type":"execute_result","data":{"text/plain":["{'kaggle (1).json': b'{\"username\":\"jmodel\",\"key\":\"9636a7ab466d9bb15529bbcc8071a87b\"}'}"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["import os, zipfile\n","\n","# Make the .kaggle directory and move kaggle.json into it\n","os.makedirs('/root/.kaggle', exist_ok=True)\n","!mv kaggle.json /root/.kaggle/\n","\n","# Set correct permissions\n","!chmod 600 /root/.kaggle/kaggle.json\n","\n","# Install Kaggle CLI\n","!pip install -q kaggle"],"metadata":{"id":"KVtrToVTsdUO","executionInfo":{"status":"ok","timestamp":1763396986760,"user_tz":480,"elapsed":4545,"user":{"displayName":"Jordan Limperis","userId":"13371126362984529793"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# List competitions just to confirm it works (optional)\n","!kaggle competitions list | head\n","\n","# Download the data for the competition\n","!kaggle competitions download -c playground-series-s5e11"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_EKl3YeXsh7F","executionInfo":{"status":"ok","timestamp":1763396991688,"user_tz":480,"elapsed":4925,"user":{"displayName":"Jordan Limperis","userId":"13371126362984529793"}},"outputId":"89c9c27c-53e4-4114-93de-26a069da2fd9"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["ref                                                                                 deadline             category              reward  teamCount  userHasEntered  \n","----------------------------------------------------------------------------------  -------------------  ---------------  -----------  ---------  --------------  \n","https://www.kaggle.com/competitions/hull-tactical-market-prediction                 2025-12-15 23:59:00  Featured         100,000 Usd       2305            True  \n","https://www.kaggle.com/competitions/vesuvius-challenge-surface-detection            2026-02-13 23:59:00  Research         100,000 Usd         98           False  \n","https://www.kaggle.com/competitions/google-tunix-hackathon                          2026-01-12 23:59:00  Featured         100,000 Usd         46           False  \n","https://www.kaggle.com/competitions/csiro-biomass                                   2026-01-28 23:59:00  Research          75,000 Usd       1209           False  \n","https://www.kaggle.com/competitions/recodai-luc-scientific-image-forgery-detection  2026-01-15 23:59:00  Research          55,000 Usd        562           False  \n","https://www.kaggle.com/competitions/MABe-mouse-behavior-detection                   2025-12-15 23:59:00  Research          50,000 Usd       1020           False  \n","https://www.kaggle.com/competitions/nfl-big-data-bowl-2026-prediction               2025-12-03 23:59:00  Featured          50,000 Usd        804           False  \n","https://www.kaggle.com/competitions/cafa-6-protein-function-prediction              2026-02-02 23:59:00  Research          50,000 Usd        678           False  \n","Downloading playground-series-s5e11.zip to /content\n","  0% 0.00/19.3M [00:00<?, ?B/s]\n","100% 19.3M/19.3M [00:00<00:00, 1.56GB/s]\n"]}]},{"cell_type":"code","source":["!unzip -o playground-series-s5e11.zip"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4CbM9SjosmYF","executionInfo":{"status":"ok","timestamp":1763396992394,"user_tz":480,"elapsed":705,"user":{"displayName":"Jordan Limperis","userId":"13371126362984529793"}},"outputId":"80edc9aa-18ce-406d-eeec-4928ae6527c8"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  playground-series-s5e11.zip\n","  inflating: sample_submission.csv   \n","  inflating: test.csv                \n","  inflating: train.csv               \n"]}]},{"cell_type":"code","source":["import pandas as pd\n","\n","train = pd.read_csv(\"train.csv\")\n","test = pd.read_csv(\"test.csv\")\n","sample_submission = pd.read_csv(\"sample_submission.csv\")\n","\n","print(train.shape, test.shape)\n","train.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":224},"id":"FdiPt4easpZi","executionInfo":{"status":"ok","timestamp":1763396993890,"user_tz":480,"elapsed":1492,"user":{"displayName":"Jordan Limperis","userId":"13371126362984529793"}},"outputId":"425733a2-24a2-4283-a15f-224487b957c6"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["(593994, 13) (254569, 12)\n"]},{"output_type":"execute_result","data":{"text/plain":["   id  annual_income  debt_to_income_ratio  credit_score  loan_amount  \\\n","0   0       29367.99                 0.084           736      2528.42   \n","1   1       22108.02                 0.166           636      4593.10   \n","2   2       49566.20                 0.097           694     17005.15   \n","3   3       46858.25                 0.065           533      4682.48   \n","4   4       25496.70                 0.053           665     12184.43   \n","\n","   interest_rate  gender marital_status education_level employment_status  \\\n","0          13.67  Female         Single     High School     Self-employed   \n","1          12.92    Male        Married        Master's          Employed   \n","2           9.76    Male         Single     High School          Employed   \n","3          16.10  Female         Single     High School          Employed   \n","4          10.21    Male        Married     High School          Employed   \n","\n","         loan_purpose grade_subgrade  loan_paid_back  \n","0               Other             C3             1.0  \n","1  Debt consolidation             D3             0.0  \n","2  Debt consolidation             C5             1.0  \n","3  Debt consolidation             F1             1.0  \n","4               Other             D1             1.0  "],"text/html":["\n","  <div id=\"df-9034dd61-0976-4160-a14e-f6a16cdc33a2\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>annual_income</th>\n","      <th>debt_to_income_ratio</th>\n","      <th>credit_score</th>\n","      <th>loan_amount</th>\n","      <th>interest_rate</th>\n","      <th>gender</th>\n","      <th>marital_status</th>\n","      <th>education_level</th>\n","      <th>employment_status</th>\n","      <th>loan_purpose</th>\n","      <th>grade_subgrade</th>\n","      <th>loan_paid_back</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>29367.99</td>\n","      <td>0.084</td>\n","      <td>736</td>\n","      <td>2528.42</td>\n","      <td>13.67</td>\n","      <td>Female</td>\n","      <td>Single</td>\n","      <td>High School</td>\n","      <td>Self-employed</td>\n","      <td>Other</td>\n","      <td>C3</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>22108.02</td>\n","      <td>0.166</td>\n","      <td>636</td>\n","      <td>4593.10</td>\n","      <td>12.92</td>\n","      <td>Male</td>\n","      <td>Married</td>\n","      <td>Master's</td>\n","      <td>Employed</td>\n","      <td>Debt consolidation</td>\n","      <td>D3</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>49566.20</td>\n","      <td>0.097</td>\n","      <td>694</td>\n","      <td>17005.15</td>\n","      <td>9.76</td>\n","      <td>Male</td>\n","      <td>Single</td>\n","      <td>High School</td>\n","      <td>Employed</td>\n","      <td>Debt consolidation</td>\n","      <td>C5</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>46858.25</td>\n","      <td>0.065</td>\n","      <td>533</td>\n","      <td>4682.48</td>\n","      <td>16.10</td>\n","      <td>Female</td>\n","      <td>Single</td>\n","      <td>High School</td>\n","      <td>Employed</td>\n","      <td>Debt consolidation</td>\n","      <td>F1</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>25496.70</td>\n","      <td>0.053</td>\n","      <td>665</td>\n","      <td>12184.43</td>\n","      <td>10.21</td>\n","      <td>Male</td>\n","      <td>Married</td>\n","      <td>High School</td>\n","      <td>Employed</td>\n","      <td>Other</td>\n","      <td>D1</td>\n","      <td>1.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9034dd61-0976-4160-a14e-f6a16cdc33a2')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-9034dd61-0976-4160-a14e-f6a16cdc33a2 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-9034dd61-0976-4160-a14e-f6a16cdc33a2');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-e9a4f3d5-a265-44f0-84e0-2e788827aa8d\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e9a4f3d5-a265-44f0-84e0-2e788827aa8d')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-e9a4f3d5-a265-44f0-84e0-2e788827aa8d button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"train"}},"metadata":{},"execution_count":6}]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0xPoLwzJq4ZY","executionInfo":{"status":"ok","timestamp":1763396999149,"user_tz":480,"elapsed":5259,"user":{"displayName":"Jordan Limperis","userId":"13371126362984529793"}},"outputId":"01378d16-dc4e-4fb0-e263-d17591767b7b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: scikit-learn<1.7.0,>=1.6.1 in /usr/local/lib/python3.12/dist-packages (1.6.1)\n","Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<1.7.0,>=1.6.1) (2.0.2)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<1.7.0,>=1.6.1) (1.16.3)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<1.7.0,>=1.6.1) (1.5.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<1.7.0,>=1.6.1) (3.6.0)\n","sklearn version: 1.6.1\n"]}],"source":["# Upgrade scikit-learn to a version that has `get_tags` (needed by AutoGluon 1.4)\n","!pip install -U \"scikit-learn>=1.6.1,<1.7.0\"\n","\n","import sklearn\n","print(\"sklearn version:\", sklearn.__version__)"]},{"cell_type":"code","source":["# Make sure xgboost is on a version AutoGluon 1.4 plays nicely with\n","!pip install -q \"xgboost<3.0.0\"\n","\n","import xgboost\n","print(\"XGBoost version:\", xgboost.__version__)  # should now be 2.x\n","\n","# Try to install AutoGluon Tabular 1.4.x with all tabular extras\n","!uv pip install autogluon"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BPTuBWcirDs7","executionInfo":{"status":"ok","timestamp":1763397038838,"user_tz":480,"elapsed":39684,"user":{"displayName":"Jordan Limperis","userId":"13371126362984529793"}},"outputId":"011379c4-3493-497b-ce8f-39804323ebe4"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.6/223.6 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hXGBoost version: 2.1.4\n","\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n","\u001b[2K\u001b[2mResolved \u001b[1m216 packages\u001b[0m \u001b[2min 2.47s\u001b[0m\u001b[0m\n","\u001b[2K\u001b[2mPrepared \u001b[1m62 packages\u001b[0m \u001b[2min 21.74s\u001b[0m\u001b[0m\n","\u001b[2mUninstalled \u001b[1m10 packages\u001b[0m \u001b[2min 1.14s\u001b[0m\u001b[0m\n","\u001b[2K\u001b[2mInstalled \u001b[1m62 packages\u001b[0m \u001b[2min 234ms\u001b[0m\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1madagio\u001b[0m\u001b[2m==0.2.6\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1maiohttp-cors\u001b[0m\u001b[2m==0.8.1\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mautogluon\u001b[0m\u001b[2m==1.4.0\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mautogluon-common\u001b[0m\u001b[2m==1.4.0\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mautogluon-core\u001b[0m\u001b[2m==1.4.0\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mautogluon-features\u001b[0m\u001b[2m==1.4.0\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mautogluon-multimodal\u001b[0m\u001b[2m==1.4.0\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mautogluon-tabular\u001b[0m\u001b[2m==1.4.0\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mautogluon-timeseries\u001b[0m\u001b[2m==1.4.0\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mboto3\u001b[0m\u001b[2m==1.40.74\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mbotocore\u001b[0m\u001b[2m==1.40.74\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mcatboost\u001b[0m\u001b[2m==1.2.8\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mcolorama\u001b[0m\u001b[2m==0.4.6\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mcolorful\u001b[0m\u001b[2m==0.5.8\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mcolorlog\u001b[0m\u001b[2m==6.10.1\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mcoreforecast\u001b[0m\u001b[2m==0.0.16\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mdistlib\u001b[0m\u001b[2m==0.4.0\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1meinx\u001b[0m\u001b[2m==0.3.0\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mevaluate\u001b[0m\u001b[2m==0.4.6\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mfugue\u001b[0m\u001b[2m==0.9.2\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mgluonts\u001b[0m\u001b[2m==0.16.2\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mjmespath\u001b[0m\u001b[2m==1.0.1\u001b[0m\n"," \u001b[31m-\u001b[39m \u001b[1mjsonschema\u001b[0m\u001b[2m==4.25.1\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mjsonschema\u001b[0m\u001b[2m==4.23.0\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mlightning\u001b[0m\u001b[2m==2.5.6\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mlightning-utilities\u001b[0m\u001b[2m==0.15.2\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mloguru\u001b[0m\u001b[2m==0.7.3\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mmlforecast\u001b[0m\u001b[2m==0.14.0\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mmodel-index\u001b[0m\u001b[2m==0.1.11\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mnlpaug\u001b[0m\u001b[2m==1.1.11\u001b[0m\n"," \u001b[31m-\u001b[39m \u001b[1mnvidia-cudnn-cu12\u001b[0m\u001b[2m==9.10.2.21\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mnvidia-cudnn-cu12\u001b[0m\u001b[2m==9.5.1.17\u001b[0m\n"," \u001b[31m-\u001b[39m \u001b[1mnvidia-cusparselt-cu12\u001b[0m\u001b[2m==0.7.1\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mnvidia-cusparselt-cu12\u001b[0m\u001b[2m==0.6.3\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mnvidia-ml-py3\u001b[0m\u001b[2m==7.352.0\u001b[0m\n"," \u001b[31m-\u001b[39m \u001b[1mnvidia-nccl-cu12\u001b[0m\u001b[2m==2.27.3\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mnvidia-nccl-cu12\u001b[0m\u001b[2m==2.26.2\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mopencensus\u001b[0m\u001b[2m==0.11.4\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mopencensus-context\u001b[0m\u001b[2m==0.1.3\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mopendatalab\u001b[0m\u001b[2m==0.0.10\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mopenmim\u001b[0m\u001b[2m==0.3.9\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mopenxlab\u001b[0m\u001b[2m==0.0.11\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1moptuna\u001b[0m\u001b[2m==4.6.0\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mordered-set\u001b[0m\u001b[2m==4.1.0\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mpdf2image\u001b[0m\u001b[2m==1.17.0\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mpy-spy\u001b[0m\u001b[2m==0.4.1\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mpycryptodome\u001b[0m\u001b[2m==3.23.0\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mpytesseract\u001b[0m\u001b[2m==0.3.13\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mpytorch-lightning\u001b[0m\u001b[2m==2.5.6\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mpytorch-metric-learning\u001b[0m\u001b[2m==2.8.1\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mray\u001b[0m\u001b[2m==2.44.1\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1ms3transfer\u001b[0m\u001b[2m==0.14.0\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mseqeval\u001b[0m\u001b[2m==1.2.2\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mstatsforecast\u001b[0m\u001b[2m==2.0.1\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mtensorboardx\u001b[0m\u001b[2m==2.6.4\u001b[0m\n"," \u001b[31m-\u001b[39m \u001b[1mtimm\u001b[0m\u001b[2m==1.0.22\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mtimm\u001b[0m\u001b[2m==1.0.3\u001b[0m\n"," \u001b[31m-\u001b[39m \u001b[1mtokenizers\u001b[0m\u001b[2m==0.22.1\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mtokenizers\u001b[0m\u001b[2m==0.21.4\u001b[0m\n"," \u001b[31m-\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.8.0+cu126\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.7.1\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mtorchmetrics\u001b[0m\u001b[2m==1.7.4\u001b[0m\n"," \u001b[31m-\u001b[39m \u001b[1mtorchvision\u001b[0m\u001b[2m==0.23.0+cu126\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mtorchvision\u001b[0m\u001b[2m==0.22.1\u001b[0m\n"," \u001b[31m-\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.57.1\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.49.0\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mtriad\u001b[0m\u001b[2m==1.0.0\u001b[0m\n"," \u001b[31m-\u001b[39m \u001b[1mtriton\u001b[0m\u001b[2m==3.4.0\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mtriton\u001b[0m\u001b[2m==3.3.1\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mutilsforecast\u001b[0m\u001b[2m==0.2.11\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mvirtualenv\u001b[0m\u001b[2m==20.35.4\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mwindow-ops\u001b[0m\u001b[2m==0.0.15\u001b[0m\n"]}]},{"cell_type":"code","source":["# ============================================================\n","# 0. IMPORTS\n","# ============================================================\n","import numpy as np\n","import pandas as pd\n","\n","from sklearn.metrics import roc_auc_score\n","from autogluon.tabular import TabularDataset, TabularPredictor\n","\n","# ============================================================\n","# 1. BASIC CONFIG\n","# ============================================================\n","TARGET = \"loan_paid_back\"\n","ID_COL = \"id\"\n","AUTOML_PATH = \"AutogluonModels_extreme\"\n","\n","# If your files are in a subdirectory (e.g. \"./ps5e11\"), set it here.\n","# Otherwise leave as \".\" for current directory.\n","DATA_DIR = \".\"\n","\n","# ============================================================\n","# 2. LOAD DATA (LOCAL FILES FROM KAGGLE API DOWNLOAD)\n","# ============================================================\n","train_path = f\"{DATA_DIR}/train.csv\"\n","test_path  = f\"{DATA_DIR}/test.csv\"\n","\n","train = pd.read_csv(train_path)\n","test  = pd.read_csv(test_path)\n","\n","# Ensure target is integer 0/1\n","train[TARGET] = train[TARGET].astype(int)\n","\n","# ============================================================\n","# 3. FEATURE ENGINEERING\n","#    Using only:\n","#    annual_income, debt_to_income_ratio, credit_score,\n","#    loan_amount, interest_rate, gender, marital_status,\n","#    education_level, employment_status, loan_purpose,\n","#    grade_subgrade\n","# ============================================================\n","\n","def add_features(df):\n","    \"\"\"\n","    EDA-driven feature engineering using the allowed columns.\n","    AutoGluon will still see the original categorical columns\n","    and handle encoding automatically.\n","    \"\"\"\n","    # 1) Ability-to-pay features\n","    eps = 1.0  # to avoid division by zero\n","    df[\"loan_to_income\"] = df[\"loan_amount\"] / (df[\"annual_income\"] + eps)\n","\n","    # 2) Log transforms to stabilize scale and capture diminishing returns\n","    df[\"log_annual_income\"] = np.log1p(df[\"annual_income\"])\n","    df[\"log_loan_amount\"] = np.log1p(df[\"loan_amount\"])\n","\n","    # 3) Interaction between debt load and interest rate\n","    df[\"dti_x_interest\"] = df[\"debt_to_income_ratio\"] * df[\"interest_rate\"]\n","\n","    # 4) Credit score band (categorical)\n","    credit_bins = [0, 580, 640, 700, 760, 900]\n","    credit_labels = [\"very_low\", \"low\", \"fair\", \"good\", \"excellent\"]\n","    df[\"credit_score_band\"] = pd.cut(\n","        df[\"credit_score\"],\n","        bins=credit_bins,\n","        labels=credit_labels,\n","        include_lowest=True\n","    )\n","\n","    return df\n","\n","# Apply feature engineering to train and test\n","train_fe = add_features(train.copy())\n","test_fe  = add_features(test.copy())\n","\n","# ============================================================\n","# 4. PREPARE DATA FOR AUTOGluon\n","# ============================================================\n","# Drop ID column so it is not used as a feature\n","train_ag = train_fe.drop(columns=[ID_COL])\n","test_ag  = test_fe.drop(columns=[ID_COL])\n","\n","# Wrap in TabularDataset (adds metadata for AutoGluon)\n","train_ag = TabularDataset(train_ag)\n","test_ag  = TabularDataset(test_ag)\n","\n","# ============================================================\n","# 5. DEFINE PREDICTOR (EXTREME PRESET)\n","# ============================================================\n","predictor = TabularPredictor(\n","    label=TARGET,\n","    eval_metric=\"roc_auc\",\n","    path=AUTOML_PATH\n",")\n","\n","# ============================================================\n","# 6. FIT WITH EXTREME PRESET\n","# ============================================================\n","# presets=\"extreme\" is the heavy, high-accuracy preset in AutoGluon v1.4.\n","# It trains many model families (GBMs, neural nets, tabular foundation models),\n","# with bagging and stacking.\n","#\n","# time_limit is total training time in seconds.\n","# ag_args_fit can control GPU usage. Here we assume 1 GPU in Colab.\n","# Set num_gpus=0 to force CPU-only.\n","\n","predictor = predictor.fit(\n","    train_data=train_ag,\n","    presets=\"extreme\",\n","    time_limit=500000,       # adjust as needed\n","    ag_args_fit={\n","        \"num_gpus\": 1      # set to 0 if you are on CPU-only\n","    }\n",")\n","\n","# ============================================================\n","# 7. INSPECT MODELS AND VALIDATION PERFORMANCE\n","# ============================================================\n","lb = predictor.leaderboard(silent=True)\n","print(lb)\n","\n","# ============================================================\n","# 8. OPTIONAL: OOF ROC AUC (AutoGluon out-of-fold predictions)\n","# ============================================================\n","try:\n","    oof_pred = predictor.predict_proba_oof(as_multiclass=False)\n","    oof_auc = roc_auc_score(train[TARGET], oof_pred)\n","    print(\"AutoGluon EXTREME OOF ROC AUC:\", round(oof_auc, 6))\n","except Exception as e:\n","    print(\"Could not compute OOF predictions (not all models are bagged).\")\n","    print(\"Error:\", e)\n","\n","# ============================================================\n","# 9. PREDICT ON TEST AND BUILD SUBMISSION\n","# ============================================================\n","test_pred_proba = predictor.predict_proba(test_ag, as_multiclass=False)\n","\n","submission = pd.DataFrame({\n","    ID_COL: test[ID_COL],\n","    TARGET: test_pred_proba.clip(0.0, 1.0)\n","})\n","\n","submission.to_csv(\"submission.csv\", index=False)\n","submission.head()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b83t14d6rHab","outputId":"52018da2-f144-44f1-f1fc-f5afce9aeb45"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Preset alias specified: 'extreme' maps to 'extreme_quality'.\n","Verbosity: 2 (Standard Logging)\n","=================== System Info ===================\n","AutoGluon Version:  1.4.0\n","Python Version:     3.12.12\n","Operating System:   Linux\n","Platform Machine:   x86_64\n","Platform Version:   #1 SMP Thu Oct  2 10:42:05 UTC 2025\n","CPU Count:          12\n","Memory Avail:       50.76 GB / 52.96 GB (95.9%)\n","Disk Space Avail:   192.71 GB / 235.68 GB (81.8%)\n","===================================================\n","Presets specified: ['extreme']\n","`extreme` preset uses a dynamic portfolio based on dataset size...\n","\tDetected data size: large (>30000 samples), using `zeroshot` portfolio (identical to 'best_quality' preset).\n","Using hyperparameters preset: hyperparameters='zeroshot'\n","Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n","Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n","DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n","\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n","\tRunning DyStack for up to 125000s of the 500000s of remaining time (25%).\n","\tRunning DyStack sub-fit in a ray process to avoid memory leakage. Enabling ray logging (enable_ray_logging=True). Specify `ds_args={'enable_ray_logging': False}` if you experience logging issues.\n","2025-11-17 16:30:44,873\tINFO worker.py:1843 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n","\t\tContext path: \"/content/AutogluonModels_extreme/ds_sub_fit/sub_fit_ho\"\n","\u001b[36m(_dystack pid=2855)\u001b[0m Running DyStack sub-fit ...\n","\u001b[36m(_dystack pid=2855)\u001b[0m Beginning AutoGluon training ... Time limit = 124996s\n","\u001b[36m(_dystack pid=2855)\u001b[0m AutoGluon will save models to \"/content/AutogluonModels_extreme/ds_sub_fit/sub_fit_ho\"\n","\u001b[36m(_dystack pid=2855)\u001b[0m Train Data Rows:    527994\n","\u001b[36m(_dystack pid=2855)\u001b[0m Train Data Columns: 16\n","\u001b[36m(_dystack pid=2855)\u001b[0m Label Column:       loan_paid_back\n","\u001b[36m(_dystack pid=2855)\u001b[0m Problem Type:       binary\n","\u001b[36m(_dystack pid=2855)\u001b[0m Preprocessing data ...\n","\u001b[36m(_dystack pid=2855)\u001b[0m Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n","\u001b[36m(_dystack pid=2855)\u001b[0m Using Feature Generators to preprocess the data ...\n","\u001b[36m(_dystack pid=2855)\u001b[0m Fitting AutoMLPipelineFeatureGenerator...\n","\u001b[36m(_dystack pid=2855)\u001b[0m \tAvailable Memory:                    51134.69 MB\n","\u001b[36m(_dystack pid=2855)\u001b[0m \tTrain Data (Original)  Memory Usage: 207.06 MB (0.4% of available memory)\n","\u001b[36m(_dystack pid=2855)\u001b[0m \tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\u001b[36m(_dystack pid=2855)\u001b[0m \tStage 1 Generators:\n","\u001b[36m(_dystack pid=2855)\u001b[0m \t\tFitting AsTypeFeatureGenerator...\n","\u001b[36m(_dystack pid=2855)\u001b[0m \tStage 2 Generators:\n","\u001b[36m(_dystack pid=2855)\u001b[0m \t\tFitting FillNaFeatureGenerator...\n","\u001b[36m(_dystack pid=2855)\u001b[0m \tStage 3 Generators:\n","\u001b[36m(_dystack pid=2855)\u001b[0m \t\tFitting IdentityFeatureGenerator...\n","\u001b[36m(_dystack pid=2855)\u001b[0m \t\tFitting CategoryFeatureGenerator...\n","\u001b[36m(_dystack pid=2855)\u001b[0m \t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n","\u001b[36m(_dystack pid=2855)\u001b[0m \tStage 4 Generators:\n","\u001b[36m(_dystack pid=2855)\u001b[0m \t\tFitting DropUniqueFeatureGenerator...\n","\u001b[36m(_dystack pid=2855)\u001b[0m \tStage 5 Generators:\n","\u001b[36m(_dystack pid=2855)\u001b[0m \t\tFitting DropDuplicatesFeatureGenerator...\n","\u001b[36m(_dystack pid=2855)\u001b[0m \tTypes of features in original data (raw dtype, special dtypes):\n","\u001b[36m(_dystack pid=2855)\u001b[0m \t\t('category', []) : 1 | ['credit_score_band']\n","\u001b[36m(_dystack pid=2855)\u001b[0m \t\t('float', [])    : 8 | ['annual_income', 'debt_to_income_ratio', 'loan_amount', 'interest_rate', 'loan_to_income', ...]\n","\u001b[36m(_dystack pid=2855)\u001b[0m \t\t('int', [])      : 1 | ['credit_score']\n","\u001b[36m(_dystack pid=2855)\u001b[0m \t\t('object', [])   : 6 | ['gender', 'marital_status', 'education_level', 'employment_status', 'loan_purpose', ...]\n","\u001b[36m(_dystack pid=2855)\u001b[0m \tTypes of features in processed data (raw dtype, special dtypes):\n","\u001b[36m(_dystack pid=2855)\u001b[0m \t\t('category', []) : 7 | ['gender', 'marital_status', 'education_level', 'employment_status', 'loan_purpose', ...]\n","\u001b[36m(_dystack pid=2855)\u001b[0m \t\t('float', [])    : 8 | ['annual_income', 'debt_to_income_ratio', 'loan_amount', 'interest_rate', 'loan_to_income', ...]\n","\u001b[36m(_dystack pid=2855)\u001b[0m \t\t('int', [])      : 1 | ['credit_score']\n","\u001b[36m(_dystack pid=2855)\u001b[0m \t2.3s = Fit runtime\n","\u001b[36m(_dystack pid=2855)\u001b[0m \t16 features in original data used to generate 16 features in processed data.\n","\u001b[36m(_dystack pid=2855)\u001b[0m \tTrain Data (Processed) Memory Usage: 39.78 MB (0.1% of available memory)\n","\u001b[36m(_dystack pid=2855)\u001b[0m Data preprocessing and feature engineering runtime = 2.44s ...\n","\u001b[36m(_dystack pid=2855)\u001b[0m AutoGluon will gauge predictive performance using evaluation metric: 'roc_auc'\n","\u001b[36m(_dystack pid=2855)\u001b[0m \tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n","\u001b[36m(_dystack pid=2855)\u001b[0m \tTo change this, specify the eval_metric parameter of Predictor()\n","\u001b[36m(_dystack pid=2855)\u001b[0m Large model count detected (110 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n","\u001b[36m(_dystack pid=2855)\u001b[0m User-specified model hyperparameters to be fit:\n","\u001b[36m(_dystack pid=2855)\u001b[0m {\n","\u001b[36m(_dystack pid=2855)\u001b[0m \t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n","\u001b[36m(_dystack pid=2855)\u001b[0m \t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n","\u001b[36m(_dystack pid=2855)\u001b[0m \t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n","\u001b[36m(_dystack pid=2855)\u001b[0m \t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n","\u001b[36m(_dystack pid=2855)\u001b[0m \t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n","\u001b[36m(_dystack pid=2855)\u001b[0m \t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\u001b[36m(_dystack pid=2855)\u001b[0m \t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\u001b[36m(_dystack pid=2855)\u001b[0m }\n","\u001b[36m(_dystack pid=2855)\u001b[0m AutoGluon will fit 2 stack levels (L1 to L2) ...\n","\u001b[36m(_dystack pid=2855)\u001b[0m Fitting 108 L1 models, fit_strategy=\"sequential\" ...\n","\u001b[36m(_dystack pid=2855)\u001b[0m Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 83308.20s of the 124993.55s of remaining time.\n","\u001b[36m(_dystack pid=2855)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=0.58%)\n","\u001b[36m(_ray_fit pid=3703)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n","\u001b[36m(_ray_fit pid=3703)\u001b[0m 1 warning generated.\n","\u001b[36m(_ray_fit pid=3703)\u001b[0m 1 warning generated.\n","\u001b[36m(_ray_fit pid=3703)\u001b[0m 1 warning generated.\n","\u001b[36m(_ray_fit pid=3703)\u001b[0m 1 warning generated.\n","\u001b[36m(_ray_fit pid=3703)\u001b[0m 1 warning generated.\n","\u001b[36m(_ray_fit pid=3703)\u001b[0m 1 warning generated.\n","\u001b[36m(_ray_fit pid=3703)\u001b[0m 1 warning generated.\n","\u001b[36m(_ray_fit pid=3703)\u001b[0m 1 warning generated.\n","\u001b[36m(_ray_fit pid=3703)\u001b[0m 1 warning generated.\n","\u001b[36m(_ray_fit pid=3703)\u001b[0m 1 warning generated.\n","\u001b[36m(_ray_fit pid=3703)\u001b[0m 1 warning generated.\n","\u001b[36m(_ray_fit pid=3703)\u001b[0m 1 warning generated.\n","\u001b[36m(_ray_fit pid=3703)\u001b[0m 1 warning generated.\n","\u001b[36m(_ray_fit pid=3703)\u001b[0m 1 warning generated.\n","\u001b[36m(_ray_fit pid=3703)\u001b[0m 1 warning generated.\n","\u001b[36m(_ray_fit pid=3703)\u001b[0m 1 warning generated.\n","\u001b[36m(_ray_fit pid=3703)\u001b[0m 1 warning generated.\n","\u001b[36m(_ray_fit pid=3703)\u001b[0m 1 warning generated.\n","\u001b[36m(_ray_fit pid=3703)\u001b[0m 1 warning generated.\n","\u001b[36m(_ray_fit pid=3703)\u001b[0m 1 warning generated.\n","\u001b[36m(_ray_fit pid=3703)\u001b[0m 1 warning generated.\n","\u001b[36m(_ray_fit pid=3703)\u001b[0m 1 warning generated.\n","\u001b[36m(_ray_fit pid=3703)\u001b[0m 1 warning generated.\n","\u001b[36m(_ray_fit pid=3703)\u001b[0m 1 warning generated.\n","\u001b[36m(_ray_fit pid=3703)\u001b[0m 1 warning generated.\n","\u001b[36m(_ray_fit pid=3703)\u001b[0m 1 warning generated.\n","\u001b[36m(_ray_fit pid=3703)\u001b[0m 1 warning generated.\n","\u001b[36m(_ray_fit pid=3703)\u001b[0m 1 warning generated.\n","\u001b[36m(_ray_fit pid=3703)\u001b[0m 1 warning generated.\n","\u001b[36m(_ray_fit pid=3703)\u001b[0m 1 warning generated.\n","\u001b[36m(_ray_fit pid=3703)\u001b[0m 1 warning generated.\n","\u001b[36m(_ray_fit pid=3703)\u001b[0m 1 warning generated.\n","\u001b[36m(_ray_fit pid=3703)\u001b[0m 1 warning generated.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[36m(_ray_fit pid=3703)\u001b[0m [1000]\tvalid_set's binary_logloss: 0.250308\n","\u001b[36m(_ray_fit pid=3703)\u001b[0m [2000]\tvalid_set's binary_logloss: 0.250115\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[36m(_ray_fit pid=4214)\u001b[0m \tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[36m(_ray_fit pid=4214)\u001b[0m [1000]\tvalid_set's binary_logloss: 0.253826\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[36m(_ray_fit pid=4579)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[36m(_ray_fit pid=4579)\u001b[0m [1000]\tvalid_set's binary_logloss: 0.254251\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[36m(_ray_fit pid=4949)\u001b[0m \tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[36m(_ray_fit pid=4949)\u001b[0m [1000]\tvalid_set's binary_logloss: 0.258725\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[36m(_ray_fit pid=5296)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[36m(_ray_fit pid=5296)\u001b[0m [1000]\tvalid_set's binary_logloss: 0.253753\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[36m(_ray_fit pid=5566)\u001b[0m \tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[36m(_ray_fit pid=5566)\u001b[0m [1000]\tvalid_set's binary_logloss: 0.252323\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[36m(_ray_fit pid=5863)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[36m(_ray_fit pid=5863)\u001b[0m [1000]\tvalid_set's binary_logloss: 0.254162\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[36m(_ray_fit pid=6229)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n","\u001b[36m(_dystack pid=2855)\u001b[0m \t0.9149\t = Validation score   (roc_auc)\n","\u001b[36m(_dystack pid=2855)\u001b[0m \t513.09s\t = Training   runtime\n","\u001b[36m(_dystack pid=2855)\u001b[0m \t72.2s\t = Validation runtime\n","\u001b[36m(_dystack pid=2855)\u001b[0m Fitting model: LightGBM_BAG_L1 ... Training model for up to 82784.85s of the 124470.19s of remaining time.\n","\u001b[36m(_dystack pid=2855)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=0.58%)\n","\u001b[36m(_ray_fit pid=6508)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[36m(_ray_fit pid=6508)\u001b[0m [1000]\tvalid_set's binary_logloss: 0.242809\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[36m(_ray_fit pid=6828)\u001b[0m \tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[36m(_ray_fit pid=6828)\u001b[0m [1000]\tvalid_set's binary_logloss: 0.245716\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[36m(_ray_fit pid=7139)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[36m(_ray_fit pid=7139)\u001b[0m [1000]\tvalid_set's binary_logloss: 0.246224\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[36m(_ray_fit pid=7399)\u001b[0m \tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[36m(_ray_fit pid=7399)\u001b[0m [1000]\tvalid_set's binary_logloss: 0.250535\n"]}]},{"cell_type":"code","source":["# ============================================================\n","# 10. SUBMIT TO KAGGLE COMPETITION\n","# ============================================================\n","# Prereqs (only need to do once per runtime):\n","# 1) You have kaggle.json in ~/.kaggle/kaggle.json\n","# 2) You ran:  !chmod 600 ~/.kaggle/kaggle.json\n","# 3) The file \"submission.csv\" exists in the current working directory.\n","\n","!pip install -q kaggle\n","\n","from kaggle.api.kaggle_api_extended import KaggleApi\n","api = KaggleApi()\n","api.authenticate()\n","\n","COMPETITION = \"playground-series-s5e11\"\n","SUBMISSION_FILE = \"submission.csv\"\n","MESSAGE = \"AutoGluon extreme in Colab\"\n","\n","print(f\"Submitting {SUBMISSION_FILE} to {COMPETITION} with message: '{MESSAGE}'\")\n","api.competition_submit(\n","    file_name=SUBMISSION_FILE,\n","    message=MESSAGE,\n","    competition=COMPETITION\n",")\n","print(\"Submission sent.\")\n"],"metadata":{"id":"luPb1ChwuGLN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!zip -r AutogluonModels_extreme.zip AutogluonModels_extreme\n","\n","from google.colab import files\n","files.download(\"AutogluonModels_extreme.zip\")\n"],"metadata":{"id":"zL9BLAX6vVbQ"},"execution_count":null,"outputs":[]}]}